Attention-based encoder-decoder neural network models have recently shown promising results in machine translation and speech recognition. In this work, we propose an attention-based neural network model for joint intent detection and slot filling, both of which are critical steps for many speech understanding and dialog systems. Unlike in machine translation and speech recognition, alignment is explicit in slot filling. We explore different strategies in incorporating this alignment information to the encoder-decoder framework. Learning from the attention mechanism in encoder-decoder model, we further propose introducing attention to the alignment-based RNN models. Such attentions provide additional information to the intent classification and slot label prediction. Our independent task models achieve state-of-the-art intent detection error rate and slot filling F1 score on the benchmark ATIS task. Our joint training model further obtains 0.56% absolute (23.8% relative) error reduction on intent detection and 0.23% absolute gain on slot filling over the independent task models.
基于注意力版的encoder-decoder神经网络模型最近在机器翻译和语音识别方面的表现的潜能不错。在这项工作中，我们提出了一个基于注意力的神经网络模型，它联合了意图推测和槽填充，这两种模型都是许多语音理解和对话系统的关键部分。与机器翻译和语音识别不同，槽填充的对齐信息是明确的。我们探索把对齐信息与encoder-decoder模型结合的不同方法。从encoder-decoder模型的注意力机制中学习，我们进一步提出了将注意力机制引入对齐的RNN模型。这种注意力给意图分类和槽填充预测提供了额外的信息。我们独立的任务模型在ATIS数据集上成为了目前意图检测错误率最低和槽填充分数最高的模型。我们的联合训练模型相比于独立的任务模型获得0.56%的绝对（23.8%的相对）误差的降低，在槽填充上提升0.23%的绝对增益?
